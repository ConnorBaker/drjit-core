#include "common.h"

/// Generate a histogram of values in the range (0..bucket_count - 1) [shared memory]
KERNEL void mkperm_phase_1_shared(const uint32_t *values, uint32_t *buckets, uint32_t size,
                                  uint32_t size_per_block, uint32_t bucket_count) {
    uint32_t *shared = SharedMemory<uint32_t>::get();

    uint32_t thread_id    = threadIdx.x,
             thread_count = blockDim.x,
             block_start  = blockIdx.x * size_per_block,
             block_end    = block_start + size_per_block;

    if (block_end > size)
        block_end = size;

    for (uint32_t i = thread_id; i < bucket_count; i += thread_count)
        shared[i] = 0;

    __syncthreads();

    for (uint32_t i = block_start + thread_id; i < block_end;
         i += thread_count) {
        uint32_t value = values[i];
        atomicAdd(shared + value, 1u);
    }

    __syncthreads();

    uint32_t *out = buckets + blockIdx.x * bucket_count;
    for (uint32_t i = thread_id; i < bucket_count; i += thread_count)
        out[i] = shared[i];
}

/// Generate a histogram of values in the range (0..bucket_count - 1) [global memory]
KERNEL void mkperm_phase_1_global(const uint32_t *values, uint32_t *buckets_, uint32_t size,
                                  uint32_t size_per_block, uint32_t bucket_count) {
    uint32_t thread_id    = threadIdx.x,
             thread_count = blockDim.x,
             block_start  = blockIdx.x * size_per_block,
             block_end    = block_start + size_per_block;

    if (block_end > size)
        block_end = size;

    uint32_t *buckets = buckets_ + blockIdx.x * bucket_count;

    for (uint32_t i = block_start + thread_id; i < block_end;
         i += thread_count) {
        uint32_t value = values[i];
        atomicAdd(buckets + value, 1u);
    }
}

/// Detect non-empty buckets and record their offsets
KERNEL void mkperm_phase_3(uint32_t *buckets, uint32_t bucket_count,
                           uint32_t size,
                           uint32_t *counter,
                           uint32_t *offsets) {
    for (uint32_t i = blockIdx.x * blockDim.x + threadIdx.x; i < bucket_count;
         i += blockDim.x * gridDim.x) {
        uint32_t offset_a = buckets[i],
                 offset_b = (i + 1 < bucket_count) ? buckets[i + 1] : size;

        if (offset_a != offset_b) {
            uint32_t k = atomicAdd(counter, 1u) * 3 + 1;
            offsets[k] = i;
            offsets[k + 1] = offset_a;
            offsets[k + 2] = offset_b - offset_a;
        }
    }
}

#if __CUDA_ARCH__ < 700
/**
 * Emulate __match_any_sync intrinsics, based on "Voting And Shuffling For
 * Fewer Atomic Operations" by Elmar Westphal.
 */
__device__ __inline__ uint32_t get_peers(uint32_t key) {
    // In the beginning, all lanes are available
    uint32_t unclaimed = 0xffffffff;

    do {
        // Find lowest-numbered unclaimed lane
        int first_unclaimed = __ffs(unclaimed) - 1;

        // Fetch its key and compare to ours
        bool match = (key == __shfl_sync(unclaimed, key, first_unclaimed));

        // Determine, which lanes had a match
        uint32_t peers = __ballot_sync(unclaimed, match);

        // Key of the current lane was chosen, return the active mask
        if (match)
            return peers;

        // Remove lanes with matching keys from the pool
        unclaimed ^= peers;
    } while (true);
}
#endif

inline __device__ uint32_t reduce(uint32_t value, uint32_t *counter) {
    // Determine lanes with a matching value
#if __CUDA_ARCH__ >= 700
    uint32_t peers = __match_any_sync(0xffffffff, value);
#else
    uint32_t peers = get_peers(value);
#endif

    // Thread's position within warp
    uint32_t lane_idx = threadIdx.x & (warpSize - 1);

    // Designate a leader thread within the set of peers
    uint32_t leader_idx  = __ffs(peers) - 1;

    // If the current thread is the leader, perform atomic op.
    uint32_t offset = 0;
    if (lane_idx == leader_idx)
        offset = atomicAdd(counter + value, __popc(peers));

    // Fetch offset into output array from leader
    offset = __shfl_sync(peers, offset, leader_idx);

    // Determine current thread's position within peer group
    uint32_t rel_pos = __popc(peers << (32 - lane_idx));

    return offset + rel_pos;
}


/// Generate a sorting permutation based on offsets generated by \ref mkperm_phase_1() [shared memory]
KERNEL void mkperm_phase_4_shared(const uint32_t *values, const uint32_t *buckets_,
                                  uint32_t *perm, uint32_t size, uint32_t size_per_block,
                                  uint32_t bucket_count) {
    uint32_t *shared = SharedMemory<uint32_t>::get();

    uint32_t thread_id    = threadIdx.x,
             thread_count = blockDim.x,
             block_start  = blockIdx.x * size_per_block,
             block_end    = block_start + size_per_block;

    if (block_end > size)
        block_end = size;

    const uint32_t *buckets = buckets_ + blockIdx.x * bucket_count;
    for (uint32_t i = thread_id; i < bucket_count; i += thread_count)
        shared[i] = buckets[i];

    __syncthreads();

    for (uint32_t i = block_start + thread_id; i < block_end;
         i += thread_count) {
        uint32_t offset = reduce(values[i], shared);
        perm[offset] = i;
    }
}



/// Generate a sorting permutation based on offsets generated by \ref mkperm_phase_1() [global memory]
KERNEL void mkperm_phase_4_global(const uint32_t *values, uint32_t *buckets_,
                                  uint32_t *perm, uint32_t size,
                                  uint32_t size_per_block, uint32_t bucket_count) {
    uint32_t thread_id    = threadIdx.x,
             thread_count = blockDim.x,
             block_start  = blockIdx.x * size_per_block,
             block_end    = block_start + size_per_block;

    if (block_end > size)
        block_end = size;

    uint32_t *buckets = buckets_ + blockIdx.x * bucket_count;

    for (uint32_t i = block_start + thread_id; i < block_end;
         i += thread_count) {
        uint32_t offset = reduce(values[i], buckets);
        perm[offset] = i;
    }
}

KERNEL void transpose(const uint32_t *in, uint32_t *out, uint32_t rows, uint32_t cols) {
    uint32_t r = blockIdx.y * blockDim.y + threadIdx.y,
             c = blockIdx.x * blockDim.x + threadIdx.x;
    if (r < rows && c < cols)
        out[r + c * rows] = in[c + r * cols];
}
