#include "common.h"

/// Generate a histogram of values in the range (0..bucket_count - 1) [shared memory]
KERNEL void mkperm_phase_1_shared(const uint32_t *values, uint32_t *buckets, uint32_t size,
                                  uint32_t size_per_block, uint32_t bucket_count) {
    uint32_t *shared = SharedMemory<uint32_t>::get();

    uint32_t thread_id    = threadIdx.x,
             thread_count = blockDim.x,
             block_start  = blockIdx.x * size_per_block,
             block_end    = block_start + size_per_block;

    if (block_end > size)
        block_end = size;

    for (uint32_t i = thread_id; i < bucket_count; i += thread_count)
        shared[i] = 0;

    __syncthreads();

    for (uint32_t i = block_start + thread_id; i < block_end;
         i += thread_count) {
        uint32_t value = values[i];
        atomicAdd(shared + value, 1);
    }

    __syncthreads();

    uint32_t *out = buckets + blockIdx.x * bucket_count;
    for (uint32_t i = thread_id; i < bucket_count; i += thread_count)
        out[i] = shared[i];
}

/// Generate a histogram of values in the range (0..bucket_count - 1) [global memory]
KERNEL void mkperm_phase_1_global(const uint32_t *values, uint32_t *buckets, uint32_t size,
                                  uint32_t size_per_block, uint32_t bucket_count) {
    uint32_t thread_id    = threadIdx.x,
             thread_count = blockDim.x,
             block_start  = blockIdx.x * size_per_block,
             block_end    = block_start + size_per_block;

    if (block_end > size)
        block_end = size;

    for (uint32_t i = block_start + thread_id; i < block_end;
         i += thread_count) {
        uint32_t value = values[i];
        atomicAdd(buckets + value, 1);
    }
}

/// Detect non-empty buckets and record their offsets
KERNEL void mkperm_phase_3(const uint32_t *buckets, uint32_t bucket_count, uint32_t size, uint32_t *offsets) {
    for (uint32_t i = blockIdx.x * blockDim.x + threadIdx.x; i < bucket_count;
         i += blockDim.x * gridDim.x) {
        uint32_t offset_a = buckets[i],
                 offset_b = (i + 1 < bucket_count) ? buckets[i + 1] : size;

        if (offset_a != offset_b) {
            uint32_t k = atomicAdd(&offsets[0], 1) * 3 + 1;
            offsets[k]     = i;
            offsets[k + 1] = offset_a;
            offsets[k + 2] = offset_b;
        }
    }
}

/// Generate a sorting permutation based on offsets generated by \ref mkperm_phase_1() [shared memory]
KERNEL void mkperm_phase_4_shared(const uint32_t *values, const uint32_t *buckets,
                                  uint32_t *perm, uint32_t size, uint32_t size_per_block,
                                  uint32_t bucket_count) {
    uint32_t *shared = SharedMemory<uint32_t>::get();

    uint32_t thread_id    = threadIdx.x,
             thread_count = blockDim.x,
             block_start  = blockIdx.x * size_per_block,
             block_end    = block_start + size_per_block;

    if (block_end > size)
        block_end = size;

    const uint32_t *in = buckets + blockIdx.x * bucket_count;
    for (uint32_t i = thread_id; i < bucket_count; i += thread_count)
        shared[i] = in[i];

    __syncthreads();

    for (uint32_t i = block_start + thread_id; i < block_end;
         i += thread_count) {
        uint32_t value = values[i],
                 index = atomicAdd(shared + value, 1);
        perm[index] = i;
    }
}

/// Generate a sorting permutation based on offsets generated by \ref mkperm_phase_1() [global memory]
KERNEL void mkperm_phase_4_global(const uint32_t *values, uint32_t *buckets_,
                                  uint32_t *perm, uint32_t size,
                                  uint32_t size_per_block, uint32_t bucket_count) {
    uint32_t thread_id    = threadIdx.x,
             thread_count = blockDim.x,
             block_start  = blockIdx.x * size_per_block,
             block_end    = block_start + size_per_block;

    if (block_end > size)
        block_end = size;

    uint32_t *buckets = buckets_ + blockIdx.x * bucket_count;

    for (uint32_t i = block_start + thread_id; i < block_end;
         i += thread_count) {
        uint32_t value = values[i],
                 index = atomicAdd(buckets + value, 1);
        perm[index] = i;
    }
}

KERNEL void transpose(uint32_t *data, uint32_t rows, uint32_t cols) {
    uint32_t r = blockIdx.y * blockDim.y + threadIdx.y,
             c = blockIdx.x * blockDim.x + threadIdx.x;

    if (r < rows && c < cols) {
        uint32_t idx0 = c + r * cols,
                 idx1 = r + c * rows,
                 val0 = data[idx0],
                 val1 = data[idx1];

        data[idx0] = val1;
        data[idx1] = val0;
    }
}
