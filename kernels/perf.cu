#include <stdint.h>
#include <stdio.h>
#include <cuda.h>
// #include <cub/cub.cuh>

#define __global___FUNC extern "C" __global__

/// Assert that a CUDA operation is correctly issued
#define cuda_check(err) cuda_check_impl(err, __FILE__, __LINE__)

void cuda_check_impl(cudaError_t errval, const char *file, const int line) {
    if (errval != cudaSuccess && errval != cudaErrorCudartUnloading)
        fprintf(stderr, "cuda_check(): runtime API error = %04d \"%s\" in "
                 "%s:%i.\n", (int) errval, cudaGetErrorName(errval), file, line);
}

void cuda_check_impl(CUresult errval, const char *file, const int line) {
    if (errval != CUDA_SUCCESS && errval != CUDA_ERROR_DEINITIALIZED) {
        const char *msg = nullptr;
        cuGetErrorString(errval, &msg);
        fprintf(stderr, "cuda_check(): API error = %04d (\"%s\") in "
                 "%s:%i.", (int) errval, msg, file, line);
    }
}


/// Generate a histogram of values in the range (0..bucket_count - 1) [shared memory, < 12K buckets typ.]
__global__ void mkperm_phase_1_small(const uint32_t *values, uint32_t *buckets, uint32_t size,
                                     uint32_t size_per_block, uint32_t bucket_count) {
    extern __shared__ uint32_t shared[];

    uint32_t thread_id    = threadIdx.x,
             thread_count = blockDim.x,
             block_start  = blockIdx.x * size_per_block,
             block_end    = block_start + size_per_block;

    if (block_end > size)
        block_end = size;

    for (uint32_t i = thread_id; i < bucket_count; i += thread_count)
        shared[i] = 0;

    __syncthreads();

    for (uint32_t i = block_start + thread_id; i < block_end;
         i += thread_count) {
        uint32_t value = values[i];
        atomicAdd(shared + value, 1);
    }

    __syncthreads();

    uint32_t *out = buckets + blockIdx.x * bucket_count;
    for (uint32_t i = thread_id; i < bucket_count; i += thread_count)
        out[i] = shared[i];
}

/// Generate a histogram of values in the range (0..bucket_count - 1) [global memory, > 12K buckets typ.]
__global__ void mkperm_phase_1_large(const uint32_t *values, uint32_t *buckets, uint32_t size,
                                     uint32_t size_per_block, uint32_t bucket_count) {
    uint32_t thread_id    = threadIdx.x,
             thread_count = blockDim.x,
             block_start  = blockIdx.x * size_per_block,
             block_end    = block_start + size_per_block;

    if (block_end > size)
        block_end = size;

    for (uint32_t i = block_start + thread_id; i < block_end;
         i += thread_count) {
        uint32_t value = values[i];
        atomicAdd(buckets + value, 1);
    }
}

/// Merge histograms from separate SMs into offsets for use by \ref mkperm_phase_3()
__global__ void mkperm_phase_2(uint32_t *buckets, uint32_t bucket_count,
                               uint32_t block_count) {
    uint32_t base = 0;
    for (int bucket = 0; bucket < bucket_count; ++bucket) {
        for (int block = 0; block < block_count; ++block) {
            uint32_t index = block * bucket_count + bucket,
                     count = buckets[index];
            buckets[index] = base;
            base += count;
        }
    }
}

/// Generate a sorting permutation based on offsets generated by \ref mkperm_phase_2() [shared memory, < 12K buckets typ.]
__global__ void mkperm_phase_3_small(const uint32_t *values, const uint32_t *buckets,
                                     uint32_t *perm, uint32_t size, uint32_t size_per_block,
                                     uint32_t bucket_count) {
    extern __shared__ uint32_t shared[];

    uint32_t thread_id    = threadIdx.x,
             thread_count = blockDim.x,
             block_start  = blockIdx.x * size_per_block,
             block_end    = block_start + size_per_block;

    if (block_end > size)
        block_end = size;

    const uint32_t *in = buckets + blockIdx.x * bucket_count;
    for (uint32_t i = thread_id; i < bucket_count; i += thread_count)
        shared[i] = in[i];

    __syncthreads();

    for (uint32_t i = block_start + thread_id; i < block_end;
         i += thread_count) {
        uint32_t value = values[i],
                 index = atomicAdd(shared + value, 1);
        perm[index] = i;
    }
}

/// Generate a sorting permutation based on offsets generated by \ref mkperm_phase_2() [global memory, > 12K buckets typ.]
__global__ void mkperm_phase_3_large(const uint32_t *values, uint32_t *buckets_,
                                     uint32_t *perm, uint32_t size,
                                     uint32_t size_per_block, uint32_t bucket_count) {
    uint32_t thread_id    = threadIdx.x,
             thread_count = blockDim.x,
             block_start  = blockIdx.x * size_per_block,
             block_end    = block_start + size_per_block;

    if (block_end > size)
        block_end = size;

    uint32_t *buckets = buckets_ + blockIdx.x * bucket_count;

    for (uint32_t i = block_start + thread_id; i < block_end;
         i += thread_count) {
        uint32_t value = values[i],
                 index = atomicAdd(buckets + value, 1);
        perm[index] = i;
    }
}


int main(int argc, char **argv) {
    uint32_t size = 1024*1024*128,
             block_count = 72,
             thread_count = 1024,
             bucket_count = 3,
             bucket_storage = bucket_count * sizeof(uint32_t);

    uint32_t size_per_block = (size + block_count - 1) / block_count;
    size_per_block = (size_per_block + thread_count - 1) / thread_count * thread_count;

    printf("Size           = %u\n", size);
    printf("Blocks         = %u\n", block_count);
    printf("Size / Block   = %u\n", size_per_block);
    printf("Bucket storage = %u\n", bucket_storage);
    printf("Shared memory  = %u\n", bucket_storage);

    uint32_t *values = nullptr, *perm = nullptr, *buckets = nullptr, *values_c = nullptr;

    values = (uint32_t *) malloc(size * sizeof(uint32_t));

    for (int i = 0; i < size; ++i) {
        uint32_t v = rand() % bucket_count;
        if (v == 3)
            v = 0;
        values[i] = v;
    }

    cuda_check(cudaMalloc(&values_c, size * sizeof(uint32_t)));
    cuda_check(cudaMalloc(&perm, size * sizeof(uint32_t)));
    cuda_check(cudaMalloc(&buckets, bucket_storage * block_count * 32));
    cuda_check(cudaMemcpy(values_c, values, size * sizeof(uint32_t), cudaMemcpyHostToDevice));

    uint32_t shared_memory = bucket_storage;

    mkperm_phase_1_tiny<<<block_count, thread_count, shared_memory * 32>>>(values_c, buckets,
            size, size_per_block, bucket_count * 32);

    cuda_check(cudaDeviceSynchronize());

    mkperm_phase_1_small<<<block_count, thread_count, shared_memory>>>(values_c, buckets,
            size, size_per_block, bucket_count);
    cuda_check(cudaDeviceSynchronize());

    mkperm_phase_1_large<<<block_count, thread_count, 0>>>(values_c, buckets,
            size, size_per_block, bucket_count);
    cuda_check(cudaDeviceSynchronize());

    // mkperm_phase_2<<<1, 1, 0>>>(buckets, bucket_count, block_count);
    //
    // mkperm_phase_3_global<<<block_count, thread_count, shared_memory>>>(values_c, buckets,
    //         perm, size, size_per_block, bucket_count);

    cuda_check(cudaFree(values_c));
    cuda_check(cudaFree(perm));
    free(values);

    return 0;
}
